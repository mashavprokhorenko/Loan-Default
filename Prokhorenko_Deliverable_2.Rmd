---
title: "Prokhorenko_Deliverable_2"
author: "Mariya Prokhorenko (Masha)"
date: "4/17/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Adjustments

## Scarrerplot Matrix after Data Adjustments
Note: see appendix for before and after.

After exploring data more and looking at the graphs I realized that my data was very skewed. I did not observe N/As or missing values, but after looking at the graphs it was evident that adjustments were needed. I studied the data and realized there were multiple outliers. For example, in the credit_sc colum, credit scores ranged from 301 to 850; however, if  credit score data was not available, the gap was filled with the number 9999. The data was therefore heavily skewed to the right. Other examples include, num_units - number of units with values 1, 2, 3, or 4 units, and 99 if not available; dtiratio - debt to income ratio with 0%<X<65%, and 999 if not available; insur_perc - insurance percentage reflects the interest rate with 0%<X<52%, and 999 if not available. I removed those rows completely to ensure the accuracy of the sampled data. 
```{r}
knitr::include_graphics("Rplot.png")
```

## Correlation Plot after Data Adjustments
Note: see the appendix for before and after.

From this correlation plot we can better see that Default vs. Interest Rate and Default vs. Credit Score are negatively correlated. We also see that Default vs. Insurance percentage and Default vs. Debt to Income Ratio are positively correlated. 
```{r}
knitr::include_graphics("Rplot2.png")
```

# Objective
To determine who will most likely default on loans and what interest rate should be charged for individual borrowers. 

# Background
Wall Street Journal speculates that risky mortgages, similar to those that caused the 2008 housing crisis, are making a comeback. Mortgage lenders then had improper incentives and originated risky loans that had a high incidence of default. My tool would decide if a mortgage is made based on objective factors that accurately predict low default potential by a borrower and what their interest rate should be. 

# Analysis
## I used the following techniques for determining interest rate:
### Linear Regression with Best Subset Selection
To adjust training error, I used the lowest value of Cp, which is an unbiased estimate of the test MSE, which selected the five-variable model as well as the lowest value of Bayesian Information criterion (BIC), which selected the five-variable model, too. We can see that Adjusted R squared flats out after 5.
```{r}
knitr::include_graphics("Rplot3.png")
```

### Linear Regression with Validation Method
I used the Validation Set Approach to choose the best model. I ran a loop six times (because the data contains six variables) to determine the best number of variables for the model.

### Regression Tree
I fit a regression tree to the data. I created a training set and fit the tree to the training set. The model used only one of the variables to construct the tree. There was no need to use cross validation to reduce complexity as the model only had two branches.
```{r}
knitr::include_graphics("Rplot4.jpg")
```

### Random Forest
I performed bagging first with six predictors. I also compared that model to the Random Forest of classification trees with two variables (i.e. squareroot of 6).

##I used the following techniques to predict default:
###Neural Network
I used the best subset selection to determine the best number of variables for the Neural Network model. To adjust training error, I used the lowest value of Cp, which is an unbiased estimate of the test MSE, which selected the five-variable model, as well as the lowest value of Bayesian Information criterion (BIC), which also selected the five-variable model. We can see that Adjusted R squared flats out after 5.
```{r}
knitr::include_graphics("Rplot5.jpg")
```

Below is the visual of the two-variable model as the five-variable model would not converge. I chose two variables to be consistent with comparisons of results for the Support Vector Machines. I used two variables Interest Rate and Insurance Percentage, because the model would not converge with other combinations, with two hidden layers of size 2 and 1, respectively. 

```{r}
knitr::include_graphics("Rplot6.jpg")
```

### Support Vector Machine Radial Kernel
I used Support Vector Machines with a radial kernel. I tested gamma equaling 0.5,2,and 4, and cost = 1.

Below is the plot of of two classes, default vs. not for two variables: credit score and interest rate; and the plot of the support vector classifier.
```{r}
knitr::include_graphics("Rplot8.jpg")
```

#### This plot shows gamma=0.5.
```{r}
knitr::include_graphics("Rplot7.jpg")
```

#### This plot shows gamma=1. Even though it predicted more correct values, it is more likely to overfit the data.
```{r}
knitr::include_graphics("Rplot10.jpg")
```

#### This plot shows gamma=4. It overfitted the data and gave lower accuracy compared to previous gammas.
```{r}
knitr::include_graphics("Rplot11.jpg")
```

### Support Vector Machine Linear Kernel
I used Support Vector Machines with a linear kernel to see if results would improve.
```{r}
knitr::include_graphics("Rplot12.jpg")
```

# Results
##Results for determining interest rate:
Below you will find the chart that summarizes performance of all models described above to determine interest rate.

| Model | MSE | Training MSE | Test MSE |
|:-----|:-----|:-----|:-----|
| Best Subset Selection | 6.169813 |   |   |
| Validation Method | 6.177905 |   |   |
| Regression Tree |   | 6.264 | 6.252 | 
| Bagging |   | 9.632 | 9.743 |
| Random Forest |   | 6.344 | 6.346 |


##Results for predicting default:
Below you will find the chart that summarizes performance of all models described above to determine default. For radial SVM, this number represents gamma = 0.5.

| Model | % Wrong | % Right |
|:-----|:-----|:-----|
| Neural Network | 24.82 | 75.17  |
| SVM Radial  | 19.86 | 80.14  |
| SVM Linear | 24.73  | 75.27 | 



# Conclusions
## Conclusions for determining interest rate:
I used a total of five methods to create five different model to estimate interest rate. The Best Subset Selection and Validation Method each produced similar results; however, Best Subset Selection uses fewer variables and has the lowest MSE. 

Therefore, the equation to predict the interest rate is

$\hat{y} = 8.533913944+ -2.493757087*default-0.004593636*credit_sc+0.007856611*insur_perc+0.234221106*num_units-1.093213175*conforming$


## Conclusions for predicting default:
I used a total of three different types of models and fitted with different parameters. The Support Vector Machine with gamma equaling 0.5 and cost parameter equaling 1 produced higher accuracy. The model was able to predict with 80.14% accuracy.

#  Follow on Work:

- Due to my computer's functional limits I used sample data. To verify my results it would be better to use the full dataset and use cloud computing to process the data.

- Create the full tool that uses the two models to estimate interest rate and predict default.

#Lessons Learned:
- Clearly defining problem statements before data cleaning.
- Examining graphs more thoroughly for outliers.
- Using more data and therefore use cloud computing to get better results for a neural network model.


# Appendix
```{r}
setwd('~/Desktop/Georgetown/542 Machine Learning/project')
x<-read.csv('df2.txt',sep="",header=T)
```
## Set up packages
```{r}
#install.packages("dplyr")
install.packages("dplyr", repos = "http://cran.us.r-project.org")
library(dplyr)
#install.packages("readr")
install.packages("readr", repos = "http://cran.us.r-project.org")
library(readr)
#install.packages("data.table")
install.packages("data.table", repos = "http://cran.us.r-project.org")
library(data.table)
#install.packages("stringr")
install.packages("stringr", repos = "http://cran.us.r-project.org")
library(stringr)
#install.packages("corrplot")
install.packages("corrplot", repos = "http://cran.us.r-project.org")
library(corrplot)
install.packages("e1071", repos = "http://cran.us.r-project.org")
library(e1071)
#install.packages("neuralnet")
install.packages("neuralnet", repos = "http://cran.us.r-project.org")
library(neuralnet)
#install.packages("leaps")
install.packages("leaps", repos = "http://cran.us.r-project.org")
library(leaps)
#install.packages("tree")
install.packages("tree", repos = "http://cran.us.r-project.org")
library(tree)
#install.packages("randomForest")
install.packages("randomForest", repos = "http://cran.us.r-project.org")
library(randomForest)
library(e1071)
```
## Explore Data
```{r}
head(x)
summary(x)
```
## Adjustments - remove outliers
```{r}
x %>%
  filter(!num_units=="99") %>%
  filter(!credit_sc=="9999") %>%
  filter(!insur_perc=="999") %>%
  filter(!dtiratio=="999") ->y
```
## Explore data after adjustments
```{r}
summary(y)
```
## Adjustments - create correlation plots with numerical columns
```{r}
y %>%
  dplyr::select(default, inter_rt, credit_sc, insur_perc, num_units, dtiratio, conforming) -> df
pairs(df, main = "Scatterplot Matrix AFTER Adjustments")

x %>%
  dplyr::select(default, inter_rt, credit_sc, insur_perc, num_units, dtiratio, conforming) -> df1
pairs(df1, main = "Scatterplot Matrix BEFORE Adjustments")
```
This looks much better compared to the one I submitted before and you can see patterns now.
## Adjustments - new correlation matrix and plot
```{r}
res<-cor(df)
head(round(res,2))
knitr::include_graphics("output_14_1.png")
corrplot(res, type = "upper", method = "color",main = "Correlation Plot AFTER Adjustments",mar=c(0,0,2,0))
```
You can see before and after correlation plot.
From this correlation plot we can better see that Default vs. Interest Rate and Default vs. Credit Score are negatively correlated. We also see that Default vs. Insurance percentage and Default vs. Debt to Income Ratio are positively correlated. 
# Analysis

## Linear Regression
### Best Subset Selection to predict interest rate
```{r}
regfit.full=regsubsets(inter_rt ~ ., df)
summary(regfit.full)
regsummary=summary(regfit.full)
names(regsummary)
regsummary$rsq
regsummary$rss

which.max(regsummary$rsq)
which.min(regsummary$rss)
which.min(regsummary$cp)
which.min(regsummary$bic)

par(mfrow=c(1,3))
plot(regsummary$cp,xlab="Number of Predictors",ylab="CP",type="l")
points(5,regsummary$cp[5],col="blue",cex=2,pch=20)
plot(regsummary$bic,xlab="Number of Predictors",ylab="BIC",type="l")
points(5,regsummary$bic[5],col="red",cex=2,pch=20)
plot(regsummary$adjr2,xlab="Number of Predictors",ylab="Adjusted RSq",type="l")
points(5,regsummary$adjr2[5],col="green",cex=2,pch=20)

```
### Fit Linear model using Best Subset Selection with 5 variables
The model with the lowest BIC is the five-variable model that contains all variables except dtiratio. We can see the coefficients associated with this model.
```{r}
which.min(regsummary$bic)
coef(regfit.full,5)
MSE.lin.regr=(1/nrow(df))*regsummary$rss[5] #to Calculate MSE
MSE.lin.regr
```
MSE for Linear model using Best Subset Selection: 6.169813

### Validation Method to estimate interest rate
```{r}
set.seed(1)
train=sample(c(TRUE, FALSE), nrow(df), rep=TRUE)
test=(!train)
regfit.best=regsubsets(inter_rt ~ ., data=df[train,], nvmax=6)
test.mat=model.matrix(inter_rt ~ ., data=df[test,])
val.errors=rep(NA,6)
for (i in 1:6){
  coefi=coef(regfit.best,id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((df$inter_rt[test]-pred)^2)
}
```


### Fit Linear model using Validation Method with 6 variables
```{r}
val.errors
which.min(val.errors)
MSE.Val.Meth=val.errors[which.min(val.errors)]
MSE.Val.Meth
coef(regfit.best,6)
```
MSE for Linear model using Validation Method: 6.177905

### Fit Regression Tree to estimate interest rate
```{r}
set.seed(1)
train=sample(1:nrow(df),nrow(df)/2)
tree.df=tree(inter_rt~.,df,subset=train)
summary(tree.df)
plot(tree.df)
text(tree.df,pretty=0)

yhat=predict(tree.df,newdata = df[-train,])
df.test=df[-train, "inter_rt"]
MSE_Regr_Tree=mean((yhat-df.test)^2)
MSE_Regr_Tree
```
MSE for Regression Tree: 6.264 - training data
MSE for Regression Tree: 6.251574 - test data

### Use Bagging to estimate interest rate
```{r}
bag.df=randomForest(inter_rt~.,data=df,subset=train,mtry=6)
bag.df
yhat.bag=predict(bag.df,newdata=df[-train,])
MSE.Rand.Forest=mean((yhat.bag-df.test)^2)
MSE.Rand.Forest
```
MSE for Bagging:9.632734 - training data
MSE for Bagging: 9.747316 - test data

### Use Random Forest to estimate interest rate
```{r}
rf.df=randomForest(inter_rt~.,data=df,subset=train,mtry=2, importance=TRUE)
rf.df
yhat.rf=predict(rf.df,newdata=df[-train,])
MSE.Rand.Forest2=mean((yhat.rf-df.test)^2)
MSE.Rand.Forest2
```
MSE for Random Forest:6.343879 - training data
MSE for Random Forest: 6.346379 - test data

### The Best Subset Selection for default
```{r}
regfit.full2=regsubsets(default ~ ., df)
summary(regfit.full2)
regsummary2=summary(regfit.full2)
names(regsummary2)
regsummary2$rsq
regsummary2$rss


which.min(regsummary2$rsq)
which.min(regsummary2$rss)
which.min(regsummary2$cp)
which.min(regsummary2$bic)

par(mfrow=c(1,3))
plot(regsummary2$cp,xlab="Number of Variables",ylab="Cp",type="l")
points(6,regsummary2$cp[6],col="blue",cex=2,pch=20)
plot(regsummary2$bic,xlab="Number of Variables",ylab="BIC",type="l")
points(5,regsummary2$bic[5],col="red",cex=2,pch=20)
plot(regsummary2$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
points(5,regsummary2$adjr2[5],col="green",cex=2,pch=20)
```
### Neural Network
```{r}
df %>%
  dplyr::select(default, inter_rt, insur_perc) -> df.nn
trainset <- df.nn[train, ]
testset <- df.nn[-train, ]
nn <- neuralnet( default ~ inter_rt+insur_perc, data=trainset, hidden=c(2,1), linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)
```
### Test the Neural Network Model
```{r}
temp_test <- subset(testset, select = c("inter_rt", "insur_perc"))
#head(temp_test)
nn.results <- compute(nn, temp_test)
#head(nn.results)
results <- data.frame(actual = testset$default, prediction = nn.results$net.result)
#head(results)
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(actual,prediction)
```
### Results
```{r}
(6049+36)/(12227+6199+6049+36)
1-(6049+36)/(12227+6199+6049+36)
```
The model yileds 75.17% accuracy with two features interest rate and insurance percentage.

#Support Vector Machines for Credit Card Default
```{r}
head(df)
df %>%
  dplyr::select(default) -> w
w=as.numeric(as.character(w$default))
plot(df[,3],df[,2],col=(3-w),xlab="Credit Score",ylab="Interest Rate")
```

### Create a dataframe for SVM
```{r}
dat=data.frame(df.vm=df[,2:3],w=as.factor(w))
head(dat)
dim(dat)
```
### Fit the traing data using Radial Support Vector Machine Model
```{r}
svmfit=svm(w~., data=dat[train,], kernel="radial")
plot(svmfit,dat[train,])
```

```{r}
summary(svmfit)
```

### Results
```{r}
summary(svmfit)
table(true=dat[-train,"w"], pred=predict(svmfit, newdata=dat[-train,]))
```
### Results
```{r}
(2332+2536)/(2332+2536+9931+9712)
1-(2332+2536)/(2332+2536+9931+9712)
```


### Fit the traing data using Radial Support Vector Machine Model with gamma =2
```{r}
svmfit=svm(w~., data=dat[train,], kernel="radial", gamma=2)
plot(svmfit,dat[train,])
table(true=dat[-train,"w"], pred=predict(svmfit, newdata=dat[-train,]))
```
Would most likely overfit, therefore will stay with gamma =1.
```{r}
(2290+2563)/(2290+2563+9973+9685)
1-(2290+2563)/(2290+2563+9973+9685)
```

### Fit the traing data using Radial Support Vector Machine Model with gamma =4
```{r}
svmfit=svm(w~., data=dat[train,], kernel="radial", gamma=4)
plot(svmfit,dat[train,])
table(true=dat[-train,"w"], pred=predict(svmfit, newdata=dat[-train,]))
```
### Results
```{r}
(2332+2536)/(2332+2536+9931+9712)
1-(2332+2536)/(2332+2536+9931+9712)
```

### Fit the traing data using Linear Support Vector Machine Model
```{r}
svmfit2=svm(w~., data=dat[train,], kernel="linear")
plot(svmfit2,dat[train,])
summary(svmfit2)
table(true=dat[-train,"w"], pred=predict(svmfit2, newdata=dat[-train,]))
```
### Results
```{r}
(2374+3688)/(2374+3688+9889+8560)
1-(2374+3688)/(2374+3688+9889+8560)
```


